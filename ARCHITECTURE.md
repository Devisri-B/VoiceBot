# Architecture

## How It Works

The voice bot uses a **real-time bidirectional audio pipeline** built on Twilio Media Streams. When a test call is initiated, the bot places an outbound call via Twilio's REST API. Twilio connects to the target number and simultaneously opens a WebSocket to our local FastAPI server (exposed via ngrok). Audio flows bidirectionally through this WebSocket: we receive the AI agent's speech as mu-law encoded 8kHz audio, and we send our patient's synthesized speech back in the same format.

The inbound audio pipeline converts mu-law to 16kHz PCM, runs silero-vad for voice activity detection, and accumulates audio in a buffer. When the VAD detects 700ms of silence following speech (indicating the agent has finished a turn), the buffer is drained and transcribed by faster-whisper running locally. The transcription, along with the scenario context and conversation history, is sent to a locally-running Ollama instance (llama3) which generates a contextually appropriate patient response. This response is converted to natural speech via edge-tts, encoded back to mu-law 8kHz, and streamed chunk-by-chunk through the WebSocket to Twilio. The system handles barge-in (agent interrupting the bot) by monitoring VAD during playback and immediately stopping if speech is detected.

## Key Design Choices

**Local-first processing**: All AI inference (STT, LLM, VAD) runs locally to avoid API costs and latency. Only edge-tts requires an internet connection (it uses Microsoft's free endpoint). This makes the system essentially free to run beyond Twilio's per-minute charges.

**Turn-taking via VAD + silence threshold**: Rather than using a fixed timer or trying to predict turn boundaries from content, we use silero-vad to detect speech boundaries and require 700ms of post-speech silence before responding. This threshold balances responsiveness (shorter = faster replies) with accuracy (longer = fewer mid-sentence interruptions). The value is configurable for tuning.

**Scenario-driven testing with LLM improvisation**: Each scenario defines a patient persona, goal, and step-by-step instructions, but the actual responses are generated by the LLM in real-time based on what the agent says. This creates realistic, varied conversations that stress-test the agent better than scripted dialogues would.

**Post-call bug detection combines rules and LLM analysis**: Rule-based detectors catch concrete issues (missing identity verification, response timing, medical safety violations) reliably, while an LLM-based review catches subtler quality issues (awkward phrasing, logical inconsistencies, missed context) that rules cannot express.
